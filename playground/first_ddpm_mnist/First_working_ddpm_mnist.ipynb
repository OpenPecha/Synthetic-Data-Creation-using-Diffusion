{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1b9259-99d6-4c89-b8db-da7992774f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "\n",
    "# Load and Preprocess Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='mnist_data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define Model and Scheduler\n",
    "class SimpleDDPM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDDPM, self).__init__()\n",
    "        self.model = UNet2DModel(\n",
    "            sample_size=32,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(64, 128, 256, 512),\n",
    "            down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "            up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.model(x, t).sample\n",
    "\n",
    "# Initialize the model and scheduler\n",
    "model = SimpleDDPM().to('cuda')\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c27c311-a18a-4357-8044-fdff4654ef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:48<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.020269332331484122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/938 [00:01<00:50, 18.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddpm_mnist_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, scheduler, criterion, optimizer, dataloader, epochs, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predicted_noise, noise)\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     22\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py:159\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    156\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     adam(\n\u001b[1;32m    169\u001b[0m         params_with_grad,\n\u001b[1;32m    170\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py:101\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m grads\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m--> 101\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Lazy state initialization\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# note(crcrpar): [special device hosting for step]\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:1059\u001b[0m, in \u001b[0;36mTensor.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1050\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1051\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1056\u001b[0m         )\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 1059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;66;03m# Do NOT handle __torch_function__ here as user's default\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;66;03m# implementation that handle most functions will most likely do it wrong.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# It can be easily overridden by defining this method on the user\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# subclass if needed.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, scheduler, criterion, optimizer, dataloader, epochs=100, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(dataloader):\n",
    "            images, _ = batch\n",
    "            images = images.to(device)\n",
    "            t = torch.randint(0, scheduler.config.num_train_timesteps, (images.size(0),), device=device).long()\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            noisy_images = scheduler.add_noise(original_samples=images, noise=noise, timesteps=t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predicted_noise = model(noisy_images, t)\n",
    "            loss = criterion(predicted_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # Save model checkpoints periodically\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"ddpm_mnist_epoch{epoch + 1}.pth\")\n",
    "\n",
    "# Run the training function\n",
    "train(model, scheduler, criterion, optimizer, train_loader, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2865b7-389f-4c18-8c24-90738aaa61b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleDDPM' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_images, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(scheduler\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_train_timesteps)):\n\u001b[0;32m----> 7\u001b[0m         noise \u001b[38;5;241m=\u001b[39m \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprev_sample\n\u001b[1;32m      9\u001b[0m     generated_images \u001b[38;5;241m=\u001b[39m (noise \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Convert [-1, 1] to [0, 1]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/schedulers/scheduling_ddpm.py:433\u001b[0m, in \u001b[0;36mDDPMScheduler.step\u001b[0;34m(self, model_output, timestep, sample, generator, return_dict)\u001b[0m\n\u001b[1;32m    429\u001b[0m t \u001b[38;5;241m=\u001b[39m timestep\n\u001b[1;32m    431\u001b[0m prev_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_timestep(t)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearned\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearned_range\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    434\u001b[0m     model_output, predicted_variance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(model_output, sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleDDPM' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Generate and Visualize Images\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    num_images = 10\n",
    "    noise = torch.randn(num_images, 1, 32, 32).to('cuda')\n",
    "    for t in reversed(range(scheduler.config.num_train_timesteps)):\n",
    "        noise = scheduler.step(model, t, noise).prev_sample\n",
    "\n",
    "    generated_images = (noise + 1) / 2  # Convert [-1, 1] to [0, 1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1, num_images, figsize=(20, 2))\n",
    "for i in range(num_images):\n",
    "    axs[i].imshow(generated_images[i].cpu().squeeze(), cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b65b49a-1a05-482e-b86c-a032dc23a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:49<1:21:15, 49.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.03971797956733593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [01:36<1:18:30, 48.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 0.021147790351020757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [02:23<1:16:37, 47.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 0.019066537970673047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [03:09<1:15:18, 47.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 0.017913723559077107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [03:56<1:14:25, 47.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.017176428591328136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [04:43<1:13:38, 47.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 0.01674911572929543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [05:30<1:12:41, 46.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 0.01653275225879446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [06:17<1:11:55, 46.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 0.016507437083322102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [07:04<1:11:27, 47.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.0160174648632913\n",
      "Epoch 10/100, Loss: 0.016041591831012322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [08:41<1:10:49, 47.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.015727979763508287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [09:28<1:09:20, 47.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.01566966178952091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [10:17<1:09:25, 47.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.015479539032540977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [11:06<1:09:16, 48.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 0.015493055608973447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [11:54<1:08:23, 48.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 0.015324854700049675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [12:41<1:06:43, 47.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Loss: 0.01522892543664778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [13:27<1:05:27, 47.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Loss: 0.015077013652453989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [14:14<1:04:38, 47.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Loss: 0.015055431869190766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [15:02<1:03:53, 47.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Loss: 0.015184384784591732\n",
      "Epoch 20/100, Loss: 0.014913661258497727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [16:38<1:02:45, 47.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Loss: 0.014924514890789414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [17:26<1:02:11, 47.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Loss: 0.014873380282286134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [18:13<1:01:04, 47.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Loss: 0.01501040482945216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [19:02<1:00:40, 47.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Loss: 0.014806035930898461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [19:50<1:00:11, 48.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 0.014830532828286322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [20:38<59:23, 48.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Loss: 0.01480949465443553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [21:27<58:33, 48.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Loss: 0.014766516278622977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [22:15<57:51, 48.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Loss: 0.014789448713442918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [23:02<56:28, 47.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Loss: 0.014643195567767758\n",
      "Epoch 30/100, Loss: 0.014638517868579991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [24:39<55:35, 48.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Loss: 0.014578459868783445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [25:28<55:00, 48.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Loss: 0.014602928186085686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [26:18<54:39, 48.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Loss: 0.014566722193927462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [27:05<53:09, 48.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Loss: 0.014600173795103297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [27:52<51:56, 47.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Loss: 0.014366539453329054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [28:40<51:08, 47.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Loss: 0.014524193485575253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [29:27<49:59, 47.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Loss: 0.0143738122108871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [30:14<49:04, 47.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Loss: 0.014503788155342725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [31:03<48:53, 48.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Loss: 0.014484303511941294\n",
      "Epoch 40/100, Loss: 0.014422788684850118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [32:39<47:11, 47.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Loss: 0.014436094775430557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [33:27<46:15, 47.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Loss: 0.014411060114714828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [34:14<45:14, 47.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Loss: 0.01415978077807025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [35:00<44:08, 47.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Loss: 0.014310292270121925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [35:13<44:50, 48.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddpm_mnist_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Generate and Visualize Images\u001b[39;00m\n\u001b[1;32m     73\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[11], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, scheduler, criterion, optimizer, dataloader, epochs, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m     47\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_tensor.py:923\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 923\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    925\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from tqdm import tqdm\n",
    "# Load and Preprocess Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='mnist_data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define Model and Scheduler\n",
    "class SimpleDDPM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDDPM, self).__init__()\n",
    "        self.model = UNet2DModel(\n",
    "            sample_size=32,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(64, 128, 256, 512),\n",
    "            down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "            up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.model(x, t).sample\n",
    "\n",
    "# Initialize the model and scheduler\n",
    "model = SimpleDDPM().to('cuda')\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Custom Training Function\n",
    "def train(model, scheduler, criterion, optimizer, dataloader, epochs=100, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            images, _ = batch\n",
    "            images = images.to(device)\n",
    "            t = torch.randint(0, scheduler.config.num_train_timesteps, (images.size(0),), device=device).long()\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            noisy_images = scheduler.add_noise(original_samples=images, noise=noise, timesteps=t)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predicted_noise = model(noisy_images, t)\n",
    "            loss = criterion(predicted_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # Save model checkpoints periodically\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), f\"ddpm_mnist_epoch{epoch + 1}.pth\")\n",
    "\n",
    "# Run the training function\n",
    "train(model, scheduler, criterion, optimizer, train_loader, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a46fcbe5-9096-4790-a2f8-3560813a1dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate and Visualize Images\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    num_images = 10\n",
    "    noise = torch.randn(num_images, 1, 32, 32).to('cuda')\n",
    "    for t in reversed(range(scheduler.config.num_train_timesteps)):\n",
    "        # Generate model output for the current timestep\n",
    "        model_output = model(noise, t)\n",
    "        # Use the scheduler to step\n",
    "        noise = scheduler.step(model_output, t, noise).prev_sample\n",
    "\n",
    "    generated_images = (noise + 1) / 2  # Convert [-1, 1] to [0, 1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1, num_images, figsize=(20, 2))\n",
    "for i in range(num_images):\n",
    "    axs[i].imshow(generated_images[i].cpu().squeeze(), cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c71116-c2d2-40d2-9b8c-6f80379f9b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
